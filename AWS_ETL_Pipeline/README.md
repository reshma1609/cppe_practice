# AWS ETL Data Pipeline using Python, S3, EC2, and RDS PostgreSQL

**Developed by:** Samiksha P  
**Institution:** Sri Manakula Vinayagar Engineering College, Puducherry  

---

## ğŸ’¡ Project Overview

This project demonstrates a complete **ETL (Extract, Transform, Load)** data pipeline built on **AWS Cloud**.  
The goal is to extract data from an Amazon **S3 bucket**, transform it using **Python and Pandas** on an **EC2 instance**, and then load the cleaned data into an **Amazon RDS (PostgreSQL)** database.

This project helped me understand how different AWS services work together to automate data movement and transformation in a real-world cloud environment.

---

## â˜ï¸ AWS Services Used

1. **Amazon S3** â€“ Stores the raw input data files (e.g., `data.csv`)
2. **Amazon EC2** â€“ Hosts and runs the Python ETL script
3. **Amazon RDS (PostgreSQL)** â€“ Stores the processed and transformed data
4. **IAM** â€“ Manages secure AWS access permissions
5. **Security Groups** â€“ Controls inbound and outbound traffic between AWS components

---

## âš™ï¸ How the ETL Pipeline Works

1. Upload the input CSV file to an **S3 bucket** (e.g., `etl-data-bucket-samiksha`).
2. Launch an **EC2 instance** and connect via SSH.
3. Run the **Python ETL script** on EC2 â€“ it connects to S3 using **Boto3** and downloads the data.
4. The data is cleaned or transformed (for example, converting height from inches to centimeters).
5. The transformed data is inserted into an **RDS PostgreSQL** database using **Psycopg2**.
6. The console displays messages after every successful step.

---

## ğŸ§° Technologies and Tools Used

- Python 3  
- Pandas  
- Boto3  
- Psycopg2  
- AWS EC2  
- AWS S3  
- AWS RDS (PostgreSQL)  
- Ubuntu 24.04 LTS  

---

## ğŸ“‚ Folder Structure

AWS_ETL_Pipeline/
â”‚
â”œâ”€â”€ etl_script.py # Main ETL Python script
â”œâ”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ README.md # Documentation file
â””â”€â”€ screenshot/ # Project screenshots
â”œâ”€â”€ Ec2_output.png
â”œâ”€â”€ S3-bucket.png
â””â”€â”€ rds-instance.png


---

## ğŸ§¾ Example Input (data.csv)

name,height_in
Alice,60
Bob,70
Charlie,65


---

## ğŸ–¥ï¸ Example Console Output

ğŸ“¥ Extracting data from S3...
âœ… Data extracted successfully!
ğŸ”§ Transforming data...
âœ… Data transformed successfully!
ğŸ“¤ Loading data into PostgreSQL...
âœ… Data loaded successfully!

yaml
Copy code

---


---

## ğŸ“ What I Learned

Through this project, I learned how to:
- Integrate **EC2, S3, and RDS** for end-to-end data workflows  
- Use **IAM roles and security groups** safely  
- Handle data transformation using **Pandas**  
- Connect and load data into **PostgreSQL** using **Psycopg2**  
- Manage and troubleshoot AWS services effectively  

This project strengthened my understanding of **data engineering** on the **AWS cloud**.

---

## ğŸš€ Future Enhancements

- Automate ETL execution using **AWS Lambda** and **EventBridge**  
- Add monitoring and error logging via **AWS CloudWatch**  
- Scale pipeline for large datasets  
- Create a dashboard for data visualization and reporting  

---

## ğŸ“¸ Project Screenshots

| Step | Description | Screenshot |
|------|--------------|-------------|
| 1ï¸âƒ£ | **EC2 Instance Running ETL Script Successfully** | ![EC2 Output](./screenshot/Ec2_output.png) |
| 2ï¸âƒ£ | **S3 Bucket with Uploaded Data File (data.csv)** | ![S3 Bucket](./screenshot/S3-bucket.png) |
| 3ï¸âƒ£ | **RDS PostgreSQL Database Connection (Endpoint Visible)** | ![RDS Instance](./screenshot/rds-instance.png) |

---

## ğŸŒ Repository Overview

This repository contains an **AWS-based ETL data pipeline** built using **Python, S3, EC2, and PostgreSQL (RDS)**.  
It demonstrates how to integrate multiple AWS services to automate data extraction, transformation, and loading into a database.

**GitHub Repository:** [https://github.com/Sampraveeen/aws-etl-pipeline](https://github.com/Sampraveeen/aws-etl-pipeline)

---

## ğŸ§  About This Project

This project was developed as part of my **academic cloud and AI practicals**, focusing on integrating cloud-based data workflows and preparing datasets for **AI-driven analytics**.

It represents a key step in understanding how cloud data pipelines work in real-world systems.

---

*This repository was created purely for educational and demonstration purposes.*  
*Â© 2025 Samiksha P*
